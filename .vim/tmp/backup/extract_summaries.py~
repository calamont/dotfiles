import os
import numpy as np
from nltk import sent_tokenize
from .extractor import PacSumExtractorWithBert
from .data_iterator import Dataset

def extract_summaries(data, beta=0.6, lambda1=0.3, lambda2=0.7, prop=0.5):
    """
    Extract summaries as the minimum number of sentences that account for some percentage of centrality in a review
    Args
        data: pandas.DataFrame
        beta (float): similarity normalisation parameter
        lambda1 (float): weight of forward-looking edges
        lambda2 (float): weight of backward-looking edges
        prop (float): proportion of centrality to account for in summaries
    """

    # check for downloaded BERT_summarisation model else download it
    path = search_download()
    BERT_CONFIG_FILE = os.path.join(path, "bert_config.json",)
    BERT_MODEL_FILE = os.path.join(path, "pytorch_bert",)
    BERT_VOCAB_FILE = os.path.join(path, "vocab.txt",)

    # Build review dataset
    review_list = [sent_tokenize(r) for r in list(data.text)]
    reviews_dataset = Dataset(review_list=review_list, vocab_file=BERT_VOCAB_FILE)
    reviews_dataset_iterator_bert = reviews_dataset.iterate_once_list_bert()

    # Init summarisation model
    extractor_bert = PacSumExtractorWithBert(
        extract_num=None,
        bert_model_file=BERT_MODEL_FILE,
        bert_config_file=BERT_CONFIG_FILE,
        beta=beta,
        lambda1=lambda1,
        lambda2=lambda2,
    )

    # score sentences by centrality
    extractions = extractor_bert.extract_summary(
        reviews_dataset_iterator_bert, 0, rouge=False
    )  # docs, summs, scores, refs, edges, pairs
    pairs = extractions[-1]

    # build summaries with sentences that account for prop % of review centrality
    tots = [np.sum([p[1] for p in pp]) for pp in pairs]
    cum_scores = [
        [np.sum([p[1] for p in pp[: i + 1]]) / tots[j] for i, p in enumerate(pp)]
        for j, pp in enumerate(pairs)
    ]
    lengths = [
        np.argwhere(np.array(cs) > prop)[0][0] + 1 if cs else 1 for cs in cum_scores
    ]

    sent_ids = [[r[0] for r in r_list][: lengths[i]] for i, r_list in enumerate(pairs)]
    summaries = [
        (" ").join([review_list[i][s] for s in s_list])
        for i, s_list in enumerate(sent_ids)
    ]
    data["extraction"] = summaries

    return data
