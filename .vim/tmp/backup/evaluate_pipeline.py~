import sys

sys.path.append("../sentence_dendrogram")
sys.path.append("../")

import argparse
import extraction
import evaluation
import filtering
import pandas as pd
from preprocessing import embed_text
from labelling import label_extractions
from tree import create_tree



def main(args):
    print("\nLoading data...")
    labelled_data = pd.read_csv(args.file)
    label_lookup = {k: v for k, v in labelled_data[["sentence", "label"]].values}

    # Create dataframe of just the reviews for extraction. This is then compared
    # to the gold standard sentence extractions.
    data = labelled_data.drop_duplicates(subset=["asin", "id", "text"]).drop(
        columns=["sentence", "label"]
    )

    print("\nExtracting data..."))
    data = getattr(extraction, args.extraction_method)(data)

    print("\nNoise filtering data..."))
    data = getattr(filtering, args.filtering_method)(data, args.min_extraction_len)

    print("\nLabelling data...")
    data = label_extractions(data, label_lookup)

    print("\nEmbedding data...")
    embeddings = embed_text(
        data["extraction"], args.tf_hub_model, args.embed_batch_size
    )
    embedding_eval_method = getattr(evaluation, args.embedding_score_method)
    # embedding_score = embedding_eval_method(embeddings, labelled_data.label.values)
    embedding_score = {}  # temporarily disabling because it takes a while
    print("Clustering...")
    tree, rootnode = create_tree(
        embeddings,
        args.linkage_method,
        args.linkage_metric,
        1000,
        5,
        data[["extraction", "id", "asin"]],
        "dist",
    )
    clustering_eval_method = getattr(evaluation, args.clustering_score_method)
    cluster_score = clustering_eval_method(tree, data)
    print(f"\nDone!\nEmbedding score: {embedding_score}\nClustering score: {cluster_score}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="A script for evaluating struggle extraction, embedding and clustering methods."
    )
    parser.add_argument(
        "file",
        type=str,
        help="Filepath for curated dataset to evaluate methods against.",
    )
    parser.add_argument(
        "--extraction_method",
        type=str,
        default="extract_sentences",
        help="Method for extracting struggles.",
    )
    parser.add_argument(
        "--filtering_method",
        type=str,
        default="length_filter",
        help="Method for filtering extractions.",
    )
    parser.add_argument(
        "--min_extraction_len",
        type=int,
        default=5,
        help="Minimum number of words in extraction (threshold for noise filtering).",
    )
    parser.add_argument(
        "--embedding_score_method",
        type=str,
        default="knn_embedding_score",
        help="Method for evaluating embeddings",
    )
    parser.add_argument(
        "--clustering_score_method",
        type=str,
        default="f1_dendrogram_score",
        help="Method for evaluating the dendrograms.",
    )
    parser.add_argument(
        "--tf_hub_model",
        type=str,
        default="https://tfhub.dev/google/universal-sentence-encoder-large/4",
        help="URL for tensorflow hub model.",
    )
    parser.add_argument("--embed_batch_size", type=int, default=100)
    parser.add_argument(
        "--linkage_method",
        type=str,
        default="ward",
        help="Method used to calculate distance between clusters (e.g. ward)",
    )
    parser.add_argument(
        "--linkage_metric",
        type=str,
        default="euclidean",
        help="Distance metric used. Defaults to euclidean.",
    )
    parser.add_argument(
        "--max_size",
        type=int,
        default=None,
        help="Maximum size of data for clustering.",
    )
    args = parser.parse_args()
    main(args)
