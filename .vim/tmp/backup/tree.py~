import os
import re
import sys
import json
import klydo

import numpy as np
import networkx as nx
import pandas as pd

from nltk.tokenize import word_tokenize
from collections import Counter, defaultdict
from scipy.cluster.hierarchy import linkage, inconsistent, to_tree
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import PorterStemmer
from nltk import word_tokenize

def create_tree(
    X, link_method, dist_metric, merge_threshold, linkage_depth, df, merge_attr="dist", size=0
):
    """Create hierarchical cluster tree"""
    print("\nClustering data...")
    links = linkage(X, method=link_method, metric=dist_metric)
    metrics = create_metric_dict(links, d=linkage_depth)
    print("\nCreating tree...")
    rootnode, nodelist = to_tree(links, rd=True)
    print("\nCreating DiGraph...")
    G = nx.DiGraph()
    for node in nodelist:
        G = add_edge(G, node, merge_threshold, links, metrics, df, merge_attr, size)
    return G, rootnode


def create_metric_dict(linkage, d):
    """Collates the metrics of a node into a dict based on the `d` links below."""
    metrics = inconsistent(linkage, d=d)
    metric_dict = defaultdict(dict)
    for i in range(len(linkage)):
        idx = i + len(linkage) + 1
        metric_dict[idx]["dist"] = linkage[i, 2]
        metric_dict[idx]["size"] = linkage[i, 3]
        metric_dict[idx]["link_mean"] = metrics[i, 0]
        metric_dict[idx]["link_std"] = metrics[i, 1]
        metric_dict[idx]["inconsistency"] = metrics[i, 3]
    return metric_dict


def add_edge(G, node, merge_threshold, linkage, metrics, df, attr="dist", size=0):
    """Adds node to NetworkX DiGraph."""
    if node.is_leaf():
        G.add_node(
            node.get_id() + size,
            node_id=node.get_id() + size,
            asin=[df.iloc[node.get_id()]["asin"]],
            review_id=[df.iloc[node.get_id()]["id"]],
            extraction=[df.iloc[node.get_id()]["extraction"]],
            children=[],
            leaf=True,
            leaf_nodes=[node.get_id() + size],
            dist=0.0,
            size=1,
            link_mean=0,
            link_std=0,
            inconsistency=0,
        )
    elif metrics[node.get_id()][attr] > merge_threshold:
        return G
    else:
        # If any child was not included in tree due to thresholding then
        # dont include parent node either.
        if not all([child.get_id() + size in G.nodes() for child in [node.get_left(), node.get_right()]]):
            return G

        left = node.get_left().get_id() + size
        right = node.get_right().get_id() + size

        G.add_node(
            node.get_id() + size,
            node_id=node.get_id() + size,
            children=[left, right],
            leaf=False,
            leaf_nodes=G.nodes[left]["leaf_nodes"]
            + G.nodes[right]["leaf_nodes"],
            dist=metrics[node.get_id()].get("dist", 0),
            size=metrics[node.get_id()].get("size", 1),
            link_mean=metrics[node.get_id()].get("link_mean", 0),
            link_std=metrics[node.get_id()].get("link_std", 0),
            inconsistency=metrics[node.get_id()].get("inconsistency", 0),
        )
        G.add_edges_from(
            [(node.get_id() + size, left), (node.get_id() + size, right)]
        )
    return G

def create_all_trees(data, embeddings, size, args, stop_words=None):
    """Convenience method to handle generating multiple trees in parallel."""

    if stop_words is None:
        stop_words = []
        
    ids = data.id.values
    asins = data.asin.values
    tree, rootnode = create_tree(
        embeddings,
        args.linkage_method,
        args.linkage_metric,
        args.merge_threshold,
        args.linkage_depth,
        data[["extraction", "id", "asin"]],
        args.threshold_attr,
        size
    )
    # tree = merge_nodes(tree, args.leaf_threshold, args.threshold_attr)
    tree = label_nodes(
        tree,
        label_method=args.label_method,
        preprocess_method="extraction",
        threshold = args.keyword_threshold, 
        n_keywords=args.n_keywords,
        stop_words=stop_words,
        keywords_key=args.keywords_key,
        size=size
    )
    return tree

def get_tree_extractions(G):
    """ Get the extractions directly from the tree graph"""
    extractions = []
    stemmer = PorterStemmer()
    for node in G.nodes.values():
        if node['leaf']:
            # use the stems of the words to later build the vocabulary of the labelling method
            stem_extrs = " ".join([stemmer.stem(token) for token in word_tokenize(node["extraction"][0])])
            extractions.append(stem_extrs)
            extractions.append(node["extraction"][0])
    return pd.DataFrame({"extraction":extractions})


def calculate_keyword_coverage(G, node, keywords_key):
    """Calculate the keyword stats for the keyword labels"""
    keywords_stats = defaultdict(int)
    for leaf in node['leaf_nodes']:
        leaf_text = G.nodes[leaf]['extraction'][0]
        node_keywords = [kw.strip() for kw in node[keywords_key].split('-') if not kw.isspace() and len(kw.strip())>0]
        if not node_keywords[0] == 'n/a':
            for kw in node_keywords:
                if kw in leaf_text:
                    keywords_stats[kw] += 1
    keywords_stats = {k: (v / len(node['leaf_nodes']))*100 for k, v in keywords_stats.items()}
    node[keywords_key+"_stats"] = keywords_stats


def label_nodes(
    G,
    label_method="TfidfVectorizer",
    preprocess_method="extraction",
    threshold = 0,
    n_keywords=5,
    stop_words=None,
    keywords_key="keywords",
    size=0
):
    """Label each node based on members of cluster."""
    print("\nLabelling nodes...")
    # get the extractions data from the tree
    ext_data = get_tree_extractions(G)
    # define the approach for generating the labels
    if label_method == "CountVectorizer":
        vectorizer = CountVectorizer(stop_words=stop_words)
    elif label_method == "TfidfVectorizer":
        vectorizer = TfidfVectorizer(stop_words=stop_words)
    counts = vectorizer.fit_transform(ext_data[preprocess_method])
    # To find word for idx position in count matrix
    idx_lookup = {v: k for k, v in vectorizer.vocabulary_.items()}
    for node in G.nodes.values():
        descendants = np.array(node["leaf_nodes"]) - size
        node_counts = np.array(counts[descendants, :].sum(axis=0)).flatten()
        freq_counts_thres = np.sort(node_counts)[::-1][:n_keywords-1] > threshold
        freq_idxs = np.argsort(node_counts)[:-n_keywords:-1][freq_counts_thres]
        if len(freq_idxs) == 0:
            freq_words = ["n/a"]
        else:
            freq_words = [idx_lookup[idx] for idx in freq_idxs]
        node[keywords_key] = "  -  ".join(freq_words) + " -" * 5
        calculate_keyword_coverage(G, node, keywords_key)
    return G



def merge_nodes(G, leaf_threshold, attr="dist"):
    """Merge descendants of node if any immediate child is a leaf."""
    merged_nodes = []
    for node_id in reversed(list(G.nodes)):
        if (node_id in merged_nodes) or (G.nodes[node_id]["leaf"]):
            continue
        if G.nodes[node_id][attr] < leaf_threshold:
            node_attrs = combine_descendants(G, node_id)
            merged_nodes.extend(node_attrs.pop("node_id"))
            nx.set_node_attributes(G, {node_id: node_attrs})
    G.remove_nodes_from(merged_nodes)
    return G


def combine_descendants(G, node_id):
    """Combine attributes of descendants of a node"""
    node_attrs = defaultdict(list)
    descendants = nx.descendants(G, node_id)
    if len(descendants) == 0:  # node_id is leaf node
        for key, val in G.nodes[node_id].items():
            try:
                node_attrs[key].extend(val)
            except TypeError:
                node_attrs[key].append(val)
    else:  # collect text of leaves below node_id
        for id in descendants:
            node = G.nodes[id]
            if node["leaf"]:
                for key, val in node.items():
                    try:
                        node_attrs[key].extend(val)
                    except TypeError:
                        node_attrs[key].append(val)
            else:
                node_attrs["node_id"].append(node["node_id"])
    # node_attrs["text"] = ", ".join(node_attrs["text"])
    node_attrs["leaf"] = True
    node_attrs.pop("dist")
    return node_attrs


def save_tree(G, save_file, local=True, save_csv=False, df=None, args=None):
    if args is None:
        args = {}
    nodelink = nx.node_link_data(G)
    print("\nSaving file to disk...")
    with open(save_file, "w") as f:
        json.dump(nodelink, f, indent=2)
    if not local:
        print("\nUploading file to s3...")
        klydo.s3.bucket.upload_data(
            save_file,
            bucket="lcv.klydo.io",
            key="data/trees/" + os.path.basename(save_file),
        )
    with open(os.path.splitext(os.path.basename(save_file))[0] + ".log", "a") as f:
        f.write("\n" + save_file + "\n")
        if args is not None:
            for k, v in vars(args).items():
                f.write(f"{k}: {v}\n")
        command = " ".join(sys.argv)
        f.write("command: " + command + "\n\n")
    if save_csv:
        save_file = os.path.splitext(save_file)[0] + "_filtered.csv"
        df.to_csv(save_file)
